\chapter{Summary \& Conclusion \label{chap:conclusion}}
  \section{Summary}
    In this project we have implemented a robust method for extracting points, meaningfully shorter content units than sentences. We made use of these in a summarization task by clustering points into cohesive sections. We then evaluated the effectiveness of our approach by comparing our summaries against those generated by a baseline statistical tool that uses sentence extraction.

    We were to meet the project's initial goals of extracting points an formulating effective summaries. Using dependency parses, we have implemented a means of extracting more useful and complete points than those in the previous project. We also improved on the detection of counter points by expanding the approach to account for antonyms --- rather than using negation terminology alone. We also implemented an approach for the identification of co-occurring points; however the results seem to be more variable and are highly dependent on large discussions where participants make many points.

    We opted to present the results of the analysis as discussion summaries. Here we were able to go beyond our goals of counter/co-occurring points and include additional sections based on the points we had extracted. There are however areas that require further work. Results from our evaluation suggest that certain presentation decisions were not always popular. We also found that while scores allocated by our bigram model correlated with those of participants, this approach in selecting extracts from clusters was not significantly better than random selection (from the same clusters) when a compared at the summary level.

    The results of the summary comparisons were very positive with all our summary types performing significantly better in comparisons against summaries of extracted sentences. We were able to reject our first two hypotheses. We were not able to reject \textbf{H3}, but have gathered useful information for future work on improving extract selection. We were unable to gather enough targeted feedback regarding sections to test \textbf{H4}, but again gathered much useful feedback for improving on the tool.

    Comments from Study 3, at the social media workshop, suggested that the summaries fell between quantitative and qualitative analysis and that they could be made more useful to researchers by quoting the (already available) ratio of counter-point sides. Comments also suggested it would be useful to select the required summary length as well as provide links back to the discussion source text. The disagreement section was referenced as being the most useful while participants found related points confusing.

  \section{Study Challenges \& Limitations}
    Limitations in the approach stem from a number of factors. Firstly, we were limited by the availability of VerbNet frames. Despite being the largest collection of its kind, we found some verbs lacking the detailed frames required for fully formed points. The generic frame is a partial fix, but the (potentially useful) annotated semantics are lost for these points.
    Another key limitation is the required corpus size. To get sufficient matches for co-occurring points the discussion has to be very large. Corpus size also relates to the issue of performance. Currently, saving parses to the graph database adds a significant performance overhead. Ideally the Cypher queries could be run on parses stored in memory without the need for an additional service.
    Finally, Argumentation Mining techniques used are limited to identifying counter/co-occurring point patterns. Comments regarding counter points from the evaluation were positive; however, there is a lot more (supporting claims, counter arguments etc.) that might be done using Argument Mining on a points dataset.

    While some technical decisions, such as choosing Docker to isolate services, worked well, others are less certain. It would have been interesting to test a dependency parser (CoreNLP or lighter alternative such as spaCy\footnote{spaCy, Multi-threaded Python NLP framework. Demo: https://spacy.io/demos/displacy}) set up within the same service as the points extractor. Doing so would reduce the HTTP request/response overhead incurred when using a parser as an external service.

    Extracting points with accompanying human-readable extracts was the most challenging task. This involved the mapping of verb frames to queries for dependency parses which took much longer than expected. Once this was in place, the point structure made the analysis used to generate the summaries relatively straightforward. Selecting negated pairs from a cluster that succinctly contradicted one another was the most challenging task in generating summaries. Designing the evaluation also took longer than planned and was conceptually very different. Supervisor input was invaluable in this exercise.

  \section{Further Work}
    The implementation is still the product of an experimental development process. Some components of the summary generation module have poor maintainability. Next steps include reducing the agglomerative complexity and repetition as well as establishing a level of unit testing. Clearer definitions for the re-formatting of extracts and extract selection (likely best expressed as a DSL) would also be beneficial for code readability. This ties into the corpus pre-processing and extract presentation, both areas that require a more integrated implementation.

    The tool chain is currently closely coupled with the corpus. We would like to establish a less bespoke approach to make analysis of new corpora easier. One interesting direction would be a simple web application that would be capable of generating a summary for a discussion of the user's choosing from \textit{Reddit}, \textit{Hacker News} or online forum. Using the tool in this way raises issue of the required discussion length. Currently a long discussion (upwards of 100,000 words) is required to extract a sufficient number of points to fill all our summary sections without repetition, and build a good sample of counter/co-occurring points. Shorter lists of extracted points can still be analyzed for counter-points; however, unless every counter point instance is of interest, a sufficient number of points are required to allow counts to reliably surface common pairs. Using shorter input texts would likely require the summary structure to be more flexible --- generating shorter summaries for shorter discussions and contextually removing sections where information was lacking.

    From the evaluation results we found that our bigram extract selection approach was not providing reliably better results than random --- though it's scores correlated with those of participants. This appeared to come down to both readability and `informativeness'. Currently extracts are weighted by the length. This was perhaps dominating the bigram score intended to represent informativeness. It would also be interesting to introduce readability as an additional factor, perhaps by incorporating an \textit{Automated readability index}. Should the tool be made accessible to the public as a web application, there would also be the opportunity to use user rating of extracts to train a model for extract preference. The task of selecting readable, informative extracts that represent the cluster is an interesting task with various options for further work.

    Currently the approach models discussions as a flat list of posts --- without reply/response annotations. Using hierarchical discussion threads opens up interesting opportunities for Argument Mining using points extraction as a basis. A new summary section that listed points commonly made in response to other points in other posts would be a valuable addition. This would also be possible with discussions on \textit{Twitter} where reply information is also available. Related to this, adding user identity information would make it possible to track a posts by the same user in discussions, and represents another interesting opportunity for further features.

    Another direction to explore would be to build a graph-like representation of the discussion. Using patterns, a graph of nodes representing nouns connected by verbs as edges could be generated. Semantic annotations from the verb frames could also be included. As part of this project we experimented with a presentation of this type, an example is included in Appendix \ref{app:disc_graph}. Related to this is the possibility of using the semantic annotations to investigate deeper abstractive summarization.

    There is also potential for further work on the summary presentation. Feedback gathered in Study 3 suggested that it would be useful to present the counts for counter points. This would allow the section to not only show a difference in opinion, but to show how opinion is split. One comment suggested that the output was in between a quantitative and qualitative report, and that adding more quantitative information to the summaries would make them more useful. Workshop attendees also suggested that being able to refer back to the source text from the extracted points would also make the results more useful --- again something that could be made possible with an online, interactive interface for summaries.

  \section{Conclusion}
    In this project we have shown that our approach is a viable foundation for summarization of discussion. We think this success can be generalized to tasks beyond summarization which also make use of text extracts. We have shown that existing tools struggle with the summarization of discursive texts, and that our summaries with sections that --- designed to give a more complete overview --- are preferred. However, summarization is just one use case for such analysis. Many varied applications enabling both search and discovery could be built using points extraction as a foundation.

    We see this project as a step forward in the process of better understanding online discussion. Our hope is that this work can become the basis for future projects and applications. From political debate to product reviews, we see this as a starting point for the exploration of a vast wealth of ideas, arguments and information.
