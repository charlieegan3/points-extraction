\chapter{Summary \& Conclusion \label{chap:conclusion}}
  \section{Summary}
    In this project we have implemented a robust method for extracting points, a meaningfully shorter content unit than a sentence. We make use of these in a summarization task by clustering points into cohesive sections. We then evaluate the effectiveness of our approach by comparing our summaries against summaries generated by a baseline statistical tool that used sentence extraction.

    We were able to meet most of the project's initial goals. Using the dependency parse, we have implemented a means of extracting more useful and complete point extracts than was implemented in the previous project. We also improved on the detection of counter points by expanding the approach to account for antonyms --- rather than negation alone. While we also implemented an approach for the identification of co-occurring points, the results seem to be more variable and are highly dependent on large discussions where participants make many points.

    We opted to present the results of the analysis as summaries of the debate. Here we were able to go beyond our goals of counter/co-occurring points and include additional sections based on the points we had extracted.

    There are however areas that require further work. Results from our evaluation suggest that certain presentation decisions we not universally popular. We also found that our means of selecting extracts from clusters was not significantly better than random, this appeared to be largely because points with poor readability scored well on the bigram model. \textcolor{red}{We were able to reject 1 (or 2/3) of our 4 hypotheses}. The evaluation did also present positive results for plain and layout summaries, giving justification for further work using points extraction as a basis.

    Comments from study 3, at the social media workshop, suggested that the summaries fell between quantitative and qualitative analysis and this they could be made more useful by quoting the (already available) ratio of counter-point sides. Comments also suggested it would be useful to select the required summary length as well as provide links back to the input text. The disagreement section was referenced as being the most useful while participants found related points confusing.

  \section{Further Work}
    The implementation is still the product of an experimental development process. The components of the summary generation approach have poor maintainability. Next steps include reducing the agglomerative complexity and repetition as well as establishing a basic level of unit testing. Clearer definitions for the re-formatting of extracts and extract selection would also be beneficial for code readability. This ties into the corpus pre-processing and extract presentation, both areas that require a more integrated implementation.

    The tool chain is currently closely coupled with the corpus. We would like to establish a more general approach to make analysis of new corpora less bespoke. One interesting direction would be a simple web application that capable of generating a summary for a discussion of the user's choosing from \textit{Reddit}, \textit{Hacker News} or online forum. This potential use case raises issue of the required discussion length. Currently a long discussion (upwards of 100,000 words) is required to extract a sufficient number of points to fill all our summary sections without repetition --- and build a good sample of counter/co-occurring points. This would likely require the summary structure to be more flexible, generating shorter summaries for shorter discussions and contextually removing sections where information was lacking.

    From the evaluation results we found that our bigram extract selection approach was not providing reliably better results than random. This appeared to come down to readability informativeness. Currently extracts are weighted by the length, perhaps this is dominating bigram score representing informativeness. It would be interesting to introduce readability as an additional factor, perhaps by incorporating an automated readability index. Should the tool be made accessible to the public as a web application there would also be the opportunity to use user rating of extracts to train a model for extract preference. The task of selecting readable, informative extracts that represent the cluster is an interesting task with many options for further work.

    Currently the approach is based a flat list of posts in a discussion without reply/response annotations. Using hierarchical discussion threads open up interesting opportunities for Argument Mining using points extraction as a basis. A new summary section that listed points commonly made in response to other points in other posts would be an interesting addition. This would also be possible with discussions on \textit{Twitter} where reply information is also exposed. Related to this, adding user identity information would also make it possible to track a posts by the same user in discussions, this represents another interesting area for further features.

    Another direction that would be inserting to explore would be to build a graph-like representation of the discussion. Using point's subject and object information a graph of nodes representing nouns connected by verbs as edges could be generated. This would be made more interesting if the semantic annotations from the verb frames could be included. As part of this project we experimented with a presentation of this type, an example is included in Appendix \ref{app:disc_graph}. Related to this is the idea of using the point's semantic annotations to to investigate abstractive summarization.

    There is also potential for further work on the summary interface. References back to source text could help explain points that require more context. A query interface where summaries could be tailored to set of reader-defined topics would also be interesting. This and many other possibilities for features and evaluation come from exposing and promoting this analysis as a public web application.

  \section{Conclusion}
    In this project we have shown that our points extraction is a viable foundation for summarization of online discussion. We think this success can be generalized to tasks beyond summarization that make use of text extracts. We have shown that existing tools struggle with the summarization of discussion and that summaries with sections that are designed to give a more complete overview are preferred. We see summarization as just one use case for such analysis and that there are many other varied layers that could be built on top of our points extraction implementation.

    We see this project a step forward in the process of better understanding online discussion. Our hope is that this work can become the basis for further work and applications that allow the exploration of the wealth of ideas and arguments currently hidden in archived threads.
